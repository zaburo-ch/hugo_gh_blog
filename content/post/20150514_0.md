+++
date = "2015-05-14T20:54:05+09:00"
title = "【Python】PageRankアルゴリズム"
categories = "つくった"
+++
PageRankというアルゴリズム、  
以前からなんとなくは知ってはいたのですが、  
ランダムサーファーモデルで計算する方法を聞いて、  
めちゃくちゃ賢いなこれーって思ったので実際にやってみました。  
  
ひとまずPageRankについて調べたことを纏めます。  
  
PageRankは基本的に次の２つの考え方でページの重要度を推定します。  
・多くのページからリンクされているページの質は高い  
・質の高いページからリンクされているページの質は高い  
  
これを数学的に考えるのにランダムサーファーモデルを利用します。  
ランダムサーファーモデルでは  
ページのリンク関係を有向グラフとして考え、  
人(サーファー)にこの有向グラフをランダムに辿らせたとき、  
人が居る確率が高いページを重要とします。  
  
まず、ページの総数をNとし、N次正方行列<strong>M</strong>を  
<strong>M</strong>[i][j] = ページjにいるサーファーがページiに移動する確率  
と定義します。  
例えばページ0にページ1とページ5へのリンクしかないとすると、  
サーファーはランダムに移動するので  
<strong>M</strong>[i][0]は i=1or5 のとき1/2 となりそれ以外のiでは0となります。  
  
また、ベクトル<strong>P</strong>(t)を時刻tに各ページに人がいる確率とすると  
<strong>P</strong>(0)は全ての要素が 1/N のベクトルとなり  
<strong>P</strong>(t+1)は<strong>M</strong>と<strong>P</strong>(t)の積を取ることで計算できます。  
  
有向グラフが強連結のとき、この遷移を無限に繰り返すことで  
<strong>P</strong>はtに依らない一定の値に収束します。よってこの<strong>P</strong>は  
<strong>M P</strong> = <strong>P</strong>  
の解を要素の和が1になるよう正規化してあげることにより求められます。  
  
この<strong>P</strong>の大きさが各ページの重要度となります。  
  
  
行列計算において <strong>Ax</strong> = λ<strong>x</strong> を解く、というのは固有値問題と言って  
色々な方法が考えられているらしいのですが、ここについては  
<a href="http://www.cms-initiative.jp/ja/events/0627yamamoto.pdf" target="_blank" title="行列計算における高速アルゴリズム">行列計算における高速アルゴリズム  
http://www.cms-initiative.jp/ja/events/0627yamamoto.pdf  
</a>こちらのページが詳しいです。  
Pythonではscipy.sparse.linalg.eigsを使うと  
Implicitly Restarted Arnoldi で計算してくれます。  
ただ、<strong>M P</strong> = <strong>P</strong> を解くだけのためにこれらを使うのが速いのかは  
勉強不足で僕もよくわかっていません。  
  
  
実際のウェブページでは、ページ間の関係を有効グラフにしても  
必ずしも上で述べたような強連結になるわけではありません。  
そのため「一定の確率でサーファーはリンクを辿らずにランダムに移動する」  
という考えを新たに導入します。その時はMにあたるものを  
全ての要素が 1/N である N次正方行列Uと普通にリンクを辿る確率αを用いて  
(α<strong>M</strong> + (1-α)<strong>U</strong>) として計算することによって<strong>P</strong>が求められます。  
  
  
次の図のようなリンク関係にあるページのPageRankを  
Pythonを利用して実際に計算してみます。  
正しく計算できれば図の通りの値が得られるはずです。  
  
<img src="/images/Linkstruct2.gif" alt="Linkstruct2.gif" border="0" width="400" height="368" />  
<span style="font-size:x-small;">  
The original uploader was <a title="wikipedia:User:Gnix" href="//en.wikipedia.org/wiki/User:Gnix">Gnix</a> at <a title="wikipedia:" href="//en.wikipedia.org/wiki/">English Wikipedia</a>[<a href="http://www.gnu.org/copyleft/fdl.html" target="_blank" title="GFDL">GFDL</a> or <a href="http://creativecommons.org/licenses/by-sa/3.0/" target="_blank" title="CC-BY-SA-3.0">CC-BY-SA-3.0</a>]</span>  
  
コードはこちら  
<a href="https://github.com/zaburo-ch/wikipedia_analysis/blob/master/pagerank.py" target="_blank" title="zaburo-ch/wikipedia_analysis/master/pagerank.py">zaburo-ch/wikipedia_analysis/master/pagerank.py</a>  
  
get_pagerankではScipyを用いて固有ベクトルを計算しています。  
get_pagerank_simpleではもっと単純に  
<strong>P</strong>(0)に何度も<strong>M</strong>をかけていき、  
かける前との差が十分小さくなるまで繰り返しています。  
  
実行結果はこんな感じです。  
<img src="/images/pagerankresult.png" alt="pagerankresult.png" border="0" width="273" height="258" />  
  
図の値とかなり一致していますね！  
2つの方法どちらもほぼ同じ値になっているので  
get_pagerank_simpleの方法でも十分実用に足りそうです。